{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostafaAshraf98/Google-Colab/blob/main/Clothes_Description_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vkdptVl8miQ8"
      },
      "outputs": [],
      "source": [
        "#----------------------------IMPORTS----------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------UNZIP FILE AT GOOGLE COLAB---------------------------------\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_name = \"Dataset 1.zip\"\n",
        "with ZipFile(file_name,'r') as zip_1:\n",
        "  zip_1.extractall()\n",
        "  print('Done')"
      ],
      "metadata": {
        "id": "T-gSPm7z43iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------- MOUNTING GOOGLE DRIVE AT GOOGLE COLAB---------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZhXTJVtyuuu",
        "outputId": "7b2ef35a-c1c9-4b7f-f8c1-f44922a7ac3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddbz6NhemiQ_",
        "outputId": "0fc71c31-a613-4912-da9c-1757cc980324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 18 classes: ['T-Shirt' 'Shoes' 'Shorts' 'Shirt' 'Pants' 'Skirt' 'Top' 'Outwear'\n",
            " 'Dress' 'Body' 'Longsleeve' 'Undershirt' 'Hat' 'Polo' 'Blouse' 'Hoodie'\n",
            " 'Skip' 'Blazer']\n"
          ]
        }
      ],
      "source": [
        "#---------------------------READING DATASET LABELS----------------------\n",
        "\n",
        "# images_path = \"D:\\Work\\CCE\\Year 4 - Senior 2\\Semester 1\\Senior-2-Semester-1\\GP\\Graduation-Project\\Clothes Description Module\\Datasets\\Dataset 1\\Images\"\n",
        "# csv_path = \"D:\\Work\\CCE\\Year 4 - Senior 2\\Semester 1\\Senior-2-Semester-1\\GP\\Graduation-Project\\Clothes Description Module\\Datasets\\Dataset 1\\CSV\\images.csv\"\n",
        "images_path = \"/content/drive/MyDrive/GP Colab/Dataset 1/Images\"\n",
        "csv_path = \"/content/drive/MyDrive/GP Colab/Dataset 1/CSV/images.csv\"\n",
        "df = pd.read_csv(csv_path) # Reading the csv file into a dataframe using pandas\n",
        "df.head() # Displaying the first 5 rows of the dataframe\n",
        "\n",
        "# # Removing all entries with label = Not sure or label Other\n",
        "df = df[df['label'] != 'Not sure']\n",
        "df = df[df['label'] != 'Other']\n",
        "\n",
        "#Remove colums: sender_id and kids\n",
        "df = df.drop(['sender_id', 'kids'], axis=1)\n",
        "df.head() # Displaying the first 5 rows of the dataframe\n",
        "\n",
        "# Extract Unique Labels = Number of classes but remove the 'Not sure' and 'Other' labels\n",
        "class_names = df['label'].unique()\n",
        "num_classes = len(class_names)\n",
        "class_ids = dict(zip(class_names, range(num_classes))) # Make a dictionary of class names and their corresponding ids\n",
        "\n",
        "print(f'There are {num_classes} classes: {class_names}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------ RESIZING IMAGES - NUMBER OF FEATURES -------------------------------------\n",
        "IMAGE_HEIGHT =  100\n",
        "IMAGE_WIDTH = 100"
      ],
      "metadata": {
        "id": "pTsf-QyiCzh_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dpQP9v6TmiRA"
      },
      "outputs": [],
      "source": [
        "#---------------------------READING IMAGES AND THEIR LABELS----------------------\n",
        "images = []\n",
        "images_labels = []\n",
        "\n",
        "#Read all the images in the file with the path images_path\n",
        "for filename in os.listdir(images_path):\n",
        "    image_path = os.path.join(images_path, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "    filename = filename.split('.')[0]\n",
        "    label = df.loc[df['image'] == filename]['label']\n",
        "    if(label.empty or image is None):\n",
        "        continue\n",
        "    label = label.values[0]\n",
        "    image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "    images.append(image)\n",
        "    images_labels.append(class_ids[label])\n",
        "        \n",
        "# Convert the images and labels to numpy arrays        \n",
        "images = np.array(images, dtype=np.float32)\n",
        "images_labels = np.array(images_labels, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3KywPOPfmiRA"
      },
      "outputs": [],
      "source": [
        "#Normalize the images\n",
        "images = images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3_PYEUCmiRB",
        "outputId": "0a876a9f-cfad-4ee4-86d9-01e68aa6e3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: 5103\n",
            "Training set size: 3571 images\n",
            "Cross-validation set size: 766 images\n",
            "Testing set size: 766 images\n"
          ]
        }
      ],
      "source": [
        "#----------------------------------------DATA SPLITTING----------------------------------------\n",
        "\n",
        "# Split the dataset into 70% for training and 15% for testing and 15% for cross-validation\n",
        "trainval_images, x_test, trainval_labels, y_test = train_test_split(images, images_labels, test_size=0.15, random_state=42)\n",
        "\n",
        "x_train, x_cv, y_train, y_cv = train_test_split(trainval_images, trainval_labels, test_size=0.15/0.85, random_state=42)\n",
        "\n",
        "m_train = len(x_train)\n",
        "m_test = len(x_test)\n",
        "m_cv = len(x_cv)\n",
        "\n",
        "# Print the size of each set\n",
        "print(f\"Dataset Size: {images.shape[0]}\")\n",
        "print(f\"Training set size: {m_train} images\")\n",
        "print(f\"Cross-validation set size: {m_cv} images\")\n",
        "print(f\"Testing set size: {m_test} images\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------DATA AUGMENTATION-----------------------------------------\n",
        "\n",
        "# Define the data augmentation configuration\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training set\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Create a new iterator from the data generator\n",
        "train_iterator = datagen.flow(x_train, y_train, batch_size=32)\n",
        "print(len(train_iterator))\n",
        "print(f'the size of the training set is {len(x_train)}')\n"
      ],
      "metadata": {
        "id": "wT_1O-cXDx6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "atIV-9UamiRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cdb1407-f325-44bf-907c-395911881041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#----------------------------------------BUILD MODEL USING TRANSFER LEARNING----------------------------------------\n",
        "\n",
        "# Use ResNet pretrained model\n",
        "# Load the pre-trained ResNet50 model without the top layer (i.e., the fully connected layers)\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
        "\n",
        "# Freeze the pre-trained layers to avoid changing their weights during training\n",
        "for layer in resnet_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add a new fully connected layer for the specific classification task\n",
        "x = Flatten()(resnet_model.output)\n",
        "x = Dense(512, activation='relu', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3),kernel_regularizer=tf.keras.regularizers.l2(0.5))(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = Dense(num_classes, activation='linear', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3),kernel_regularizer=tf.keras.regularizers.l2(0.5))(x)\n",
        "\n",
        "# Create a new model by combining the pre-trained ResNet50 model with the new fully connected layers\n",
        "model = Model(inputs=resnet_model.input, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-eRV3I2miRC"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------BUILD MODEL USING CNN----------------------------------------\n",
        "\n",
        "# Create a convolutional neural network (CNN)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3), name = 'Conv1'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max1'),\n",
        "    tf.keras.layers.Conv2D(64, (5, 5), padding = 'same', activation='relu', name = 'Conv2'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max2'),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', name = 'Conv3'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max3'),\n",
        "    tf.keras.layers.Conv2D(512, (5, 5), strides = 2, activation='relu', name = 'Conv4'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max4'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Dense1'),    \n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Dense2'),    \n",
        "    tf.keras.layers.Dense(len(class_names), activation='linear', name = 'Dense3')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDvLxBxtmiRC"
      },
      "outputs": [],
      "source": [
        "#-----------------------------------------------TRAINING----------------------------------------\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "# the training process will stop if the validation loss does not improve after 4 epochs.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "# his callback can be used to save the best model weights during training based on a given metric (e.g., validation accuracy or loss).\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=30, validation_data=(x_cv, y_cv),callbacks=[early_stopping,checkpoint])\n",
        "# history = model.fit(x_train, y_train, epochs=20, batch_size=30, validation_data=(x_cv, y_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIFHrCNtmiRD"
      },
      "outputs": [],
      "source": [
        "#----------------------------------DELETE MODEL----------------------------------\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSosIXmomiRD"
      },
      "outputs": [],
      "source": [
        "#-----------------------------------SAVE MODEL-----------------------------------\n",
        "# model_path = 'clothing_detector.pkl'\n",
        "model_path = \"/content/drive/MyDrive/GP Colab/clothing_detector.pkl\"\n",
        "pickle.dump(model, open(model_path, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------LOADING MODEL FROM CHECKPOINT-------------------------\n",
        "model.load_weights('best_model.h5')"
      ],
      "metadata": {
        "id": "RlQgQpCQDy3o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------- LOADING MODEL FROM PICKL FILE ------------------------\n",
        "# Load the saved model from a .pkl file\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "2EOrJ035EooH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AavzLaiTmiRD",
        "outputId": "20a2bd66-43ea-407a-8c1c-9dc682a1c11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112/112 [==============================] - 5s 30ms/step\n",
            "24/24 [==============================] - 1s 26ms/step\n",
            "24/24 [==============================] - 1s 27ms/step\n",
            "Train Accuracy = 99.30 % with 3571 training examples\n",
            "CV Accuracy = 84.33 % with 766 cv examples\n",
            "Test Accuracy = 80.94 % with 766 test examples\n"
          ]
        }
      ],
      "source": [
        "#----------------------------------------PREDICTIONS----------------------------------------\n",
        "train_predictions = model.predict(x_train)\n",
        "cv_predictions = model.predict(x_cv)\n",
        "test_predictions = model.predict(x_test)\n",
        "\n",
        "train_count_correct = 0\n",
        "cv_count_correct = 0\n",
        "test_count_correct = 0\n",
        "\n",
        "for i in range(m_train):\n",
        "    predicted = np.argmax(train_predictions[i])\n",
        "    if (predicted == y_train[i]):\n",
        "        train_count_correct += 1\n",
        "train_accuracy = train_count_correct / m_train\n",
        "\n",
        "for i in range(m_test):\n",
        "    predicted = np.argmax(test_predictions[i])\n",
        "    if (predicted == y_test[i]):\n",
        "        test_count_correct += 1\n",
        "test_accuracy = test_count_correct / m_test\n",
        "\n",
        "for i in range(m_cv):\n",
        "    predicted = np.argmax(cv_predictions[i])\n",
        "    if (predicted == y_cv[i]):\n",
        "        cv_count_correct += 1\n",
        "cv_accuracy = cv_count_correct / m_cv\n",
        "\n",
        "print(\"Train Accuracy = %.2f\" % (train_accuracy*100),'% with', m_train, 'training examples')\n",
        "print(\"CV Accuracy = %.2f\" % (cv_accuracy*100),'% with', m_cv, 'cv examples')\n",
        "print(\"Test Accuracy = %.2f\" % (test_accuracy*100),'% with', m_test, 'test examples')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "104451bbd5af0301cd2444e0cd70e99d9901e7de2f5bbc69a1b6555b99a1a266"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}